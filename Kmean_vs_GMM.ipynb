{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: \n",
    "# comparazione tra algoritmi di clustering (k-mean vs GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parlando di algoritmi di Machine Learning, questi vengono principalmente suddivisi in due grandi famiglie: apprendimento supervisionato e apprendimento non supervisionato. \n",
    "Il primo passo per la soluzione di un problema é capire se il modello che andremo ad implementare utilizzerá degli algoritmi di apprendimento automatico facenti parte del primo gruppo o del secondo (anche non di rado potrebbero volerci entrambi). Per farlo, occorrerá studiare il set di dati che ci viene fornito, che di solito si compone di una serie di caratteristiche (features) e, opzionalmente, una lista di output. \n",
    "\n",
    "Per rendere le cose semplici, possiamo dire che se nel nostro dataset sono presenti delle variabili di output dovremo orientarci su uno o piú algoritmi di apprendimento supervisionato, altrimenti la strada da percorrere sará quella dell'apprendimento non supervisionato. \n",
    "Facciamo un paio di esempi: \n",
    "1) Dallo storico dei dati di vendita di un negozio vorrei predire le vendite nel prossimo mese. Questo é un esempio di apprendimento supervisionato;\n",
    "2) Dato un grande database di clienti (con il loro relativo storico di acquisti) vorrei definire dei gruppi in base a comportamenti di acquisto simili. Non conoscendo a priori queste proprietá comportamentali, utilizzeró degli algoritmi di apprendimento non supervisionato.\n",
    "\n",
    "Tra gli algoritmi di apprendimento non supervisionato, quelli piú comuni sono gli algoritmi di clustering, usati appunto per raggruppare i dati di storico in varie classi. Lo scopo é individuare i centroidi di questi cluster, in modo da poter calcolare con semplicitá a quale classe appartiene il prossimo dato che viene buttato in pasto al nostro modello. I due algoritmi principali usati per questo scopo sono: K-Means e i Gaussian Mixture Models (GMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: Dataset\n",
    "### Genero un dataset di blobs randomici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo script che segue genera un grafico con dei punti (800) sparsi in modo casuale. \n",
    "Per le nostre elaborazioni, ipotizziamo di dividere il nostro dataset in tre cluster distinti.\n",
    "La funzione make_blobs prende come input il numero di oggetti che vogliamo generare e il numero di cluster su cui dividerli (n_samples e centers). Come output, ci fornirá X, un array bidimensionale (ogni elemento rappresenta le coordinate x e y dell' oggetto generato casualmente in modo da poterlo disegnare come punto su un piano cartesiano), e un array monodimensionale y_true con il numero del cluster al quale appartiene quell' oggetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=800, centers=4)\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo algoritmo di clustering non supervisionato che andremo ad analizzare si chiama K-Means. Il suo compito é quello di assegnare a ogni oggetto di un set di dati multidimensionale (bidimensionale in questo caso) il suo cluster di appartenenza (sapendo a priori il numero di cluster su cui dividere i dati). Per farlo, utilizza i seguenti concetti:\n",
    "1) il centroide é la media aritmetica tra tutti gli oggetti facenti parte di quel cluster\n",
    "2) un oggetto facente parte di un cluster é piú vicino al centroide di quel cluster rispetto a tutti gli altri centroidi.\n",
    "\n",
    "Usando la tipica API di predizione di scikit-learn, lasceremo all'algoritmo il compito di trovare il centro di ogni cluster e di conseguenza assegnare ogni punto al cluster con il centroide piú vicino. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=40, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diamo un'occhio alla performace dell' algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit -n 1000 kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "198 microsecondi. Non é un errore. Se vi state chiedendo come abbia fatto a fare tanti calcoli in cosí poco tempo la risposta é che in realtá non li ha fatti, almeno non tutti. Questo é l'algoritmo su cui si basa k-mean:\n",
    "\n",
    "- Setta i centri dei cluster a caso\n",
    "- Ripete fino alla convergenza:\n",
    "    - assegna i punti al cluster con il centroide piú vicino\n",
    "    - ricalcola i centri dei cluster usando la media dei punti di ogni cluster \n",
    "\n",
    "Spesso, a ogni iterazione dell'algoritmo, il risultato sará una sempre migliore stima delle caratteristiche del cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GMM\n",
    "gmm = GMM(n_components=4).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Come per K-means, ho usato l'API di scikit-learn usando l'algoritmo chiamato GMM (Gaussian Mixture Models). \n",
    "\n",
    "Il risultato é simile, anche se non identico, infatti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Gli oggetti sono assegnati agli stessi cluster nei due algoritmi? {}\".format(all(labels == y_kmeans)))\n",
    "print(\"Quanti sono quelli diversi (su 800)? {}\".format(len([x for x, y in zip(labels, y_kmeans) if x != y])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma quanto ci mette a fare i calcoli?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit -n 1000 gmm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo osservare, questo algoritmo é molto piú lento di k-means. A sua discolpa, GMM fa qualche considerazione in piú, e ci aiuta a calcolare i cluster in alcuni casi in cui k-means non ci riesce. Quali sono questi casi? \n",
    "\n",
    "Guardiamo il dataset con l'occhio umano: mentre per alcuni punti possiamo dire con esattezza a quale cluster appartiene, per altri il confine diventa piú sfocato. K-means non conosce questo significato, quindi cercheró di spiegare in modo semplice come ragiona: immaginate di disegnare un cerchio avente centro il centroide del cluster e come raggio la distanza tra il centroide e l'oggetto piú lontano di quel cluster. Per k-mean, ogni oggetto al di fuori di questo cerchio, non fa parte del cluster.\n",
    "\n",
    "Facciamo un esempio grafico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # plot the input data\n",
    "    ax = ax or plt.gca()\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kmeans(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo approccio non funziona su un set di dati fatto per esempio cosí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=1)\n",
    "plot_kmeans(kmeans, X_stretched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso, piú che un raggruppamento su una figura circolare, ci saremmo aspettatti un raggruppamento su base ellittica per esempio. Guardiamo la differenza con l'elaborazione di GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "gmm = GMM(n_components=4, covariance_type='full', random_state=42)\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 5):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "\n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "        \n",
    "plot_gmm(gmm, X_stretched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
